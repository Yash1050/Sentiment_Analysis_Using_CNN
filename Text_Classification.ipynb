{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinandan2024/Sentiment_Analysis_Using_CNN/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow.python.keras as keras\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pydot"
      ],
       "metadata": {
        "id": "z3nUDH3PI2AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "#policy = mixed_precision.Policy('mixed_float16')\n",
        "#mixed_precision.set_policy(policy)"
      ],
      "metadata": {
        "id": "SoLN6aatJCcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")"
      ],
      "metadata": {
        "id": "UyX25zVdL9wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us define methods to pre-process the tweets**\n"
      ],
      "metadata": {
        "id": "g4bQ0VdMOUyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_url(text):\n",
        "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return url_pattern.sub(r'', text)\n",
        " # converting return value from list to string\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text ):\n",
        "    delete_dict = {sp_character: '' for sp_character in string.punctuation}\n",
        "    delete_dict[' '] = ' '\n",
        "    table = str.maketrans(delete_dict)\n",
        "    text1 = text.translate(table)\n",
        "    #print('cleaned:'+text1)\n",
        "    textArr= text1.split()\n",
        "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))])\n",
        "\n",
        "    return text2.lower()"
      ],
      "metadata": {
        "id": "8FqOnxteOepo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us pre-process the data**"
      ],
      "metadata": {
        "id": "Cq5P8gx-Ra1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data= pd.read_csv(\"/content/train.csv\")\n",
        "train_data.dropna(axis = 0, how ='any',inplace=True)\n",
        "train_data['Num_words_text'] = train_data['text'].apply(lambda x:len(str(x).split()))\n",
        "mask = train_data['Num_words_text'] >2\n",
        "train_data = train_data[mask]\n",
        "print('-------Train data--------')\n",
        "print(train_data['sentiment'].value_counts())\n",
        "print(len(train_data))\n",
        "print('-------------------------')\n",
        "max_train_sentence_length  = train_data['Num_words_text'].max()\n",
        "\n",
        "\n",
        "train_data['text'] = train_data['text'].apply(remove_emoji)\n",
        "train_data['text'] = train_data['text'].apply(remove_url)\n",
        "train_data['text'] = train_data['text'].apply(clean_text)\n",
        "\n",
        "\n",
        "test_data= pd.read_csv(\"/content/test.csv\")\n",
        "test_data.dropna(axis = 0, how ='any',inplace=True)\n",
        "test_data['Num_words_text'] = test_data['text'].apply(lambda x:len(str(x).split()))\n",
        "\n",
        "max_test_sentence_length  = test_data['Num_words_text'].max()\n",
        "\n",
        "mask = test_data['Num_words_text'] >2\n",
        "test_data = test_data[mask]\n",
        "\n",
        "print('-------Test data--------')\n",
        "print(test_data['sentiment'].value_counts())\n",
        "print(len(test_data))\n",
        "print('-------------------------')\n",
        "\n",
        "test_data['text'] = test_data['text'].apply(remove_emoji)\n",
        "test_data['text'] = test_data['text'].apply(remove_url)\n",
        "test_data['text'] = test_data['text'].apply(clean_text)\n",
        "\n",
        "\n",
        "print('Train Max Sentence Length :'+str(max_train_sentence_length))\n",
        "print('Test Max Sentence Length :'+str(max_test_sentence_length))\n",
        "\n",
        "#all_sentences = train_data['text'].tolist() + test_data['text'].tolist()"
      ],
      "metadata": {
        "id": "D2QKSrgSRilQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.duplicated().sum()"
      ],
      "metadata": {
        "id": "SHJVWP7mfABs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us split the training data into train and validation datasets Let us convert our training,validation and test data into the format accepted by tensorflow**"
      ],
      "metadata": {
        "id": "ZjbcYMozSnDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words,oov_token=\"unk\")\n",
        "tokenizer.fit_on_texts(train_data['text'].tolist())\n",
        "\n",
        "\n",
        "print(str(tokenizer.texts_to_sequences(['xyz how are you'])))"
      ],
      "metadata": {
        "id": "W0tYFxoIR3ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(train_data['text'].tolist(),\\\n",
        "                                                      train_data['sentiment'].tolist(),\\\n",
        "                                                      test_size=0.1,\\\n",
        "                                                      stratify = train_data['sentiment'].tolist(),\\\n",
        "                                                      random_state=0)\n",
        "\n",
        "\n",
        "print('Train data len:'+str(len(X_train)))\n",
        "print('Class distribution'+str(Counter(y_train)))\n",
        "print('Valid data len:'+str(len(X_valid)))\n",
        "print('Class distribution'+ str(Counter(y_valid)))\n",
        "\n",
        "\n",
        "x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
        "x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
        "x_test  = np.array( tokenizer.texts_to_sequences(test_data['text'].tolist()) )\n",
        "\n",
        "\n",
        "\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=40)\n",
        "x_valid = pad_sequences(x_valid, padding='post', maxlen=40)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=40)\n",
        "\n",
        "print(x_train[0])\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "train_labels = le.fit_transform(y_train)\n",
        "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))\n",
        "#print(train_labels)\n",
        "valid_labels = le.transform(y_valid)\n",
        "valid_labels = np.asarray( tf.keras.utils.to_categorical(valid_labels))\n",
        "\n",
        "test_labels = le.transform(test_data['sentiment'].tolist())\n",
        "test_labels = np.asarray(tf.keras.utils.to_categorical(test_labels))\n",
        "list(le.classes_)\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test,test_labels))"
      ],
      "metadata": {
        "id": "PQtbzNXvStvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[:10])\n",
        "train_labels = le.fit_transform(y_train)\n",
        "print('Text to number')\n",
        "print(train_labels[:10])\n",
        "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))\n",
        "print('Number to category')\n",
        "print(train_labels[:10])"
      ],
      "metadata": {
        "id": "5l1_Q-5YUCXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count =0\n",
        "print('======Train dataset ====')\n",
        "for value,label in train_ds:\n",
        "    count += 1\n",
        "    print(value,label)\n",
        "    if count==3:\n",
        "        break\n",
        "count =0\n",
        "print('======Validation dataset ====')\n",
        "for value,label in valid_ds:\n",
        "    count += 1\n",
        "    print(value,label)\n",
        "    if count==3:\n",
        "        break\n",
        "print('======Test dataset ====')\n",
        "for value,label in test_ds:\n",
        "    count += 1\n",
        "    print(value,label)\n",
        "    if count==3:\n",
        "        break"
      ],
      "metadata": {
        "id": "HTM7UZ8nUlza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us create a simple Convolutional Neural Network model**\n",
        "\n"
      ],
      "metadata": {
        "id": "OAtg31CHVzmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features =20000\n",
        "embedding_dim =64\n",
        "sequence_length = 40\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\n",
        "                                    embeddings_regularizer = regularizers.l2(0.0005)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv1D(128,3, activation='relu',\\\n",
        "                                 kernel_regularizer = regularizers.l2(0.0005),\\\n",
        "                                 bias_regularizer = regularizers.l2(0.0005)))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(3, activation='sigmoid',\\\n",
        "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
        "                                bias_regularizer=regularizers.l2(0.001),))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), optimizer='Nadam', metrics=[\"CategoricalAccuracy\"])"
      ],
      "metadata": {
        "id": "RNz1BfrqVcnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's train our model**"
      ],
      "metadata": {
        "id": "yLHhg0G8aDYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "# Fit the model using the train and test datasets.\n",
        "#history = model.fit(x_train, train_labels,validation_data= (x_test,test_labels),epochs=epochs )\n",
        "history = model.fit(train_ds.shuffle(2000).batch(128),\n",
        "                    epochs= epochs ,\n",
        "                    validation_data=valid_ds.batch(128),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "ZXkOcHUWWDPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "_IbI3hIqWv0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label=' training data')\n",
        "plt.plot(history.history['val_loss'], label='validation data)')\n",
        "plt.title('Loss for Text Classification')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NcN8Od-WauUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Categorical Accuracy for Text Classification')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OJhAIkjFbXTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6ZB6UUfbYlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
